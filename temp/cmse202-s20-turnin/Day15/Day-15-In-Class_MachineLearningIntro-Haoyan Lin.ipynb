{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <p style=\"text-align: right;\"> &#9989; **Haoyan Lin** </p>\n",
    "#### <p style=\"text-align: right;\"> &#9989; Larissa Ford, Thanh lai, Connor Kromp, Dongdong Li</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 15 In-Class Assignment: Introduction to Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Scientific motivation** \n",
    "    - Classifying data (iris types) \n",
    "2. **Modeling tools** \n",
    "    - Machine Learning (Perceptron)\n",
    "3. **Programming concepts** \n",
    "    - Creating Classes and re-usable code\n",
    "    - Pulling in data from outside sources \n",
    "    - Using external libraries \n",
    "    \n",
    "\n",
    "### Agenda for today's class\n",
    "\n",
    "</p>\n",
    "\n",
    "1. Review of pre-class assignment\n",
    "1. Problem Statement\n",
    "1. Basics of the perceptron model\n",
    "1. Loading and inspecting the data\n",
    "1. Building the perceptron model\n",
    "1. Plotting the decision boundary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Perceptron Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perceptron is one of the first used examples of what has come to be called a neural network. Invented in 1958, it was originally hailed as a way to achieve what had come to be called \"Artificial Intelligence\". However, it was quickly proved that the perceptron model was limited. The claims and subsequent refutations halted neural network research for a number of years.\n",
    "\n",
    "Perceptrons are used as a kind of **classifier** that we can train using examples. The simplest perceptron is what is known as a binary classifier. By this we mean that, we can provide individual examples of two classes (remember, a binary classifier) where the individuals are represented by some number of features/inputs. All examples use the same input features, but the particular feature values are used for the classification process. The goal is the create the classifier such that, when a never-seen-before individual is provided to the perceptron, it can correctly determine that individual's class\n",
    "\n",
    "There are ways to extend the perceptron's ability to deal with multiple classes (to classify the inputs as representing one of `n` classes instead of only two), but for this exercise we will only be concerned with a binary classifer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The limitations of a binary perceptron are that the classes must be **linearly separable**. It is easier to show than to explain. Look at the two graphs below. The axes represent the range of values for the two input features. The dots represent individual input examples based on their corresponding feature values and the colors represent the class that each individual example belongs to. \n",
    "\n",
    "<img src=\"https://i.imgur.com/pU70IHB.png\">\n",
    "\n",
    "For experiment A, it is clear that we can draw a line through the 2D input/feature space such that we can separate the examples of the two classes. For experiment B, no such line separating the two classes exists. Furthermore, it is also clear that we could draw **many** lines for A such that we separate the two classes. \n",
    "\n",
    "The limitations therefore are:\n",
    "- a perceptron can only classify elements that are linearly separable\n",
    "- a perceptron cannot distinguish which linear boundary is \"better\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It's a line\n",
    "\n",
    "The way to think about a perceptron then is that it is learning a line through the feature space. We will discuss that in a minute but accepting that is true, what is a line? A line is just a simple equation with the following form:\n",
    "\n",
    "$$y = mx + b $$\n",
    "\n",
    "Where `m` is the slope and `b` is the intercept. We are going to train our perceptron such that it finds `m` and `b` where that particular line separates two classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The setup\n",
    "\n",
    "We are going to setup our perceptron as shown in the image below:\n",
    "\n",
    "<img src=\"https://i.imgur.com/gz05jKe.png\">\n",
    "\n",
    "Remembering that each individual example is represented as a set of features, a **vector** of features, we provide a **weight** associated with each feature input. When the features of a individual example is provided, we multipy each input by its associated weight and sum those products together. \n",
    "\n",
    "You may think of this process as multiplying two vectors (the input vector and the weight vector) together, index to index, and summing the resulting products as what is called the **dot product**. You might note that `numpy` provides such a function.\n",
    "\n",
    "Having obtained the dot product of feature inputs and weights, we add what is called a **bias term**. The bias is also associated with a weight (the constant 1 at the top of the inputs) though it's value is constant for all inputs. Altogether the dot product represents the `m` of our line equation and the bias represents the `b` of that equation. \n",
    "\n",
    "Having obtained the result, we pass that result through an **activation function**. Our activation function is typically a step function such that it yields either 1 or -1 indicating whether the particular input is part of one class (1) or the other (-1). Remember, binary classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Loading and inspecting the data\n",
    "\n",
    "Before we build a machine learning model, we need data to base it off of. The data set we are going to use has been provided for you in the directory for this assignment, `binary-iris.csv`. It is a variation on a classic, simple classification data set for iris flowers from 1936. It is used often as a very simple test of learning systems. The original (see https://en.wikipedia.org/wiki/Iris_flower_data_set ) has 4 classes and 4 inputs, for our experiments we have modified the file to have only 2 inputs and 2 classes. \n",
    "\n",
    "**Load the data into python and visualize (with a plot) to get a sense for what it looks like. Use different colors to represent the two different iris classifications.**\n",
    "\n",
    "As an aside, try to do it without use Pandas. Let's see if we can get used to opening files and manipulating the strings we get from the file. If that proves too difficult, go ahead and use Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Do This: Load in the iris.csv file and plot the data based on the iris classifications\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "sepal_length, sepal_width = np.loadtxt(\"binary-iris.csv\", usecols = (0,1), unpack = True, skiprows = 1, delimiter = ',')\n",
    "\n",
    "\n",
    "plt.scatter(sepal_length, sepal_width, color = \"y\",alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Perceptron class (first cut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a perceptron class but let's skip the learning process for the moment. Here's what we need before adding learning:\n",
    "- The class needs a `__init__` method. \n",
    "  - It takes as arguments the labeled data (the data features plus a label at the end)\n",
    "\n",
    "  - it creates an instance variable `data` for the data from the file. It should contain 2D vector where each row represents an example. The row should consists of two features and a class. The classes should be modified for 1 (iris setosa) and -1 (iris versicolor). \n",
    "\n",
    "  - it creates a 1D weight vector `weights` of the same shape as the number of features + 1. The **plus 1** is for the bias weight. For now, fill the weights with some value (say 1.0). We'll fix that later. \n",
    "  - it creates a constant bias value `bias`\n",
    "\n",
    "- The class needs a `predict` method which takes a single argument, an array of features from a single example in the data set. It will do the following:\n",
    "   - multipy (as a dot product) the argument feature vector and the weights\n",
    "   - multiply the bias by its weight. Add that to the result.\n",
    "   - Apply the activation to the result to say whether the class of that input is -1 vs 1\n",
    "   - return that predicted class value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## write your class code here\n",
    "\n",
    "\n",
    "class Perceptron():\n",
    "     def __init__(self, weights, bias):\n",
    "         self.w = weights\n",
    "         self.b = bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run your code with the main below. See if it works, even though it is not yet a very good classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'bias'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-1db4f9397743>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPerceptron\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"weights: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bias: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'bias'"
     ]
    }
   ],
   "source": [
    "## get data from file, just using file and string ops\n",
    "f = open(\"binary-iris.csv\")\n",
    "header = next(f) # dump the header line\n",
    "data = []\n",
    "for line in f:\n",
    "    fields = line.split(\",\")\n",
    "    # need to strip label because, as the last element, it has a \\n\n",
    "    label = (1.0 if fields[2].strip() == \"Iris-setosa\" else -1.0)\n",
    "    # the fields are strings until we conver them\n",
    "    data.append([float(fields[0]), float(fields[1]), label])\n",
    "f.close()\n",
    "\n",
    "p = Perceptron(data)\n",
    "print(\"weights: \", p.weights)\n",
    "print(\"bias: \", p.bias)\n",
    "print(\"prediction: \",p.predict([1,1])) # some arbitrary feature vector, just testing here\n",
    "print(p.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning\n",
    "\n",
    "OK, now the interesting part. We need to learn the value of the weights, including the value of the bias weight, so that the predictions the `predict` method makes are good. How to do that?\n",
    "\n",
    "The basic idea is this. We feed in the data that we have where we know the labels (and we do) to `predict`. We then compare the classification we want (from the existing data) and the classification we got (from the `predict` method). We use that difference to update **all the weights**. We do this for **each** of the data. \n",
    "\n",
    "However, we need to do one more thing. We need to not **over correct** the weights. If we do that then the weight values might swing wildly over time and never settle down. So we also provide a `learning_rate`. This rate reduces how much the weight changes. Overall then we use the following equation:\n",
    "\n",
    "$$ self.weights[i] = self.weights[i] +  self.learning\\_rate * (class\\_label - prediction\\_label) * input[i] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do this for each input of the example and each corresponding weight. We also include the bias weight. We do this for some number of iterations because, with a small `learning_rate`, we need to repeat the process to get the weight values correctly set. We also update the bias weight each time we update a weight. We use the same difference, `class_label-prediction_label` for that update (even though there is no label for the bias). If `weights[0]` is the bias weight, then that equation would be\n",
    "\n",
    "$$ self.weights[0] = self.weights[0] + learning\\_rate * (class\\_label - prediction\\_ label) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify our Perceptron class\n",
    "\n",
    "We need to update our `Perceptron` class to deal with learning. Copy the class you wrote from above in the cell below and make the following changes:\n",
    "\n",
    "- `__init__` besides labeled data, it takes two more parameters: the number of iterations of learning you want and the learning_rate. They should be also stored as instance variables\n",
    "- a method called `fit`. It performs the number of iterations of learning provided in `__init__` on all the labeled data as described\n",
    "- method called `errors`. It will print out the number of errors on predicted vs actual class labels and the weights at the end of the run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "## write your updated class code here\n",
    "class Perception():\n",
    "    def _init_(self, fit,errors):\n",
    "        self.f = fit\n",
    "        self.e = errors\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run your code with the code below. Change the number of iterations, the learning rate, and see what happens. Make sure `errors` prints out the weights as you will need them below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() takes 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-bd0b26be4d6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPerceptron\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() takes 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "f = open(\"binary-iris.csv\")\n",
    "header = next(f) # dump the header line\n",
    "data = []\n",
    "for line in f:\n",
    "    fields = line.split(\",\")\n",
    "    # need to strip label because, as the last element, it has a \\n\n",
    "    label = (1.0 if fields[2].strip() == \"Iris-setosa\" else -1.0)\n",
    "    # the fields are strings until we conver them\n",
    "    data.append([float(fields[0]), float(fields[1]), label])\n",
    "f.close()\n",
    "    \n",
    "p = Perceptron(data, 10, 0.01)\n",
    "p.fit()\n",
    "p.errors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the decision boundary\n",
    "\n",
    "If you have made it this far, we can try and visualize this.\n",
    "\n",
    "Once we have run the math, we can try and plot the decision boundary as well as the data entries. Let's do some more math. **Remember** the actual bias is the bias constant you used time the bias weight (typically the first weight)\n",
    "\n",
    "Our basic calculation is $w \\cdot x + b = 0 $ where `w` and `x` are vectors and the operation $\\cdot$ is the dot product. Since this is in two dimensions (two features for an input), we can rewrite this as: $w_1 * x_1 + w_2 * x_2 + b = 0$. \n",
    "\n",
    "That looks a lot like an equation of a line in the form of $Ax + By - C = 0$ if we assume $x_1 == x$ and $x_2 == y$. Let's isolate the x and y intercept:\n",
    "\n",
    "$ x = \\frac{-(b - w_2*y)}{w_1} $ but if $y=0$ (which it does at the x intercept) then $x = \\frac{-b}{w_1}$\n",
    "\n",
    "$ y = \\frac{-(b - w_1*x)}{w_2}$ but if $x=0$ (which it does at the y intercept) then $y = \\frac{-b}{w_2}$\n",
    "\n",
    "We get the following then:\n",
    "\n",
    "$slope = -\\frac{w_1}{w_2}$ and $intercept = \\frac{-b}{w_2}$\n",
    "\n",
    "Plug in your weights and plot the line. Plot the data as well where they are colored to indicate which class they belong to.\n",
    "\n",
    "The data should be stored in the `Perceptron` instance. Get the weights (which `error` should print out) and plot the line. The separate the two classes and plot each as a different color.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## write your code here to plot the decision boundary. Use the weights from above\n",
    "figure = plt.figure(figsize=(12, 10))\n",
    "b = 0.02\n",
    "w = 1\n",
    "# iterate over datasets\n",
    "for ds in f:\n",
    "    \n",
    "    X, y = ds\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4)\n",
    "    \n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    # just plot the dataset 'f' first\n",
    "    cm = plt.cm.jet\n",
    "    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "    ax = plt.subplot(len(f), len(treeclassifiers) + 1, i)\n",
    "    # Plot the training points\n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, alpha=0.7)\n",
    "    # and testing points\n",
    "    # ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6)\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    i += 1\n",
    " \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "### Congratulations, we're done!\n",
    "\n",
    "Now, you just need to submit this assignment by uploading it to the course <a href=\"https://d2l.msu.edu/\">Desire2Learn</a> web page for today's submission folder (Don't forget to add your names in the first cell).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#169; Copyright 2019,  Michigan State University Board of Trustees"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
